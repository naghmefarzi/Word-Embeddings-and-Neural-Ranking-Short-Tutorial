### Topics Covered:

  Neural ranking vs traditional lexical methods
  
  Word embeddings: One-hot, Word2Vec, GloVe
  
  Simple ranking using average embeddings
  
  Transformer-based embeddings (e.g., BERT)
  
  Neural ranking models:
    
    Bi-Encoder
    
    Cross-Encoder

    ColBERT


### Includes:

Slide-style lecture notes (PDF/Markdown format)

Colab notebook with code demonstrations of word embeddings and ranking techniques
